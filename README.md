# Deep Research Multi-Agent System with RAG via HITL

> Enterprise-grade multi-agent research pipeline orchestrated with LangGraph, designed to accelerate technical decision-making by automating web research, knowledge curation, and report generation. Features semantic memory via Chroma with Human-in-the-Loop curation, session persistence via PostgreSQL, and full observability via LangSmith.
>
> The system classifies user intent, routes between new research and existing knowledge, searches and extracts relevant content from the web, generates comprehensive Markdown summaries, and stores curated context in a vector store pending HITL approval. Inference powered by Mercury 2 (Inception Labs) — a state-of-the-art diffusion LLM capable of 1,009 tokens/second with 1.7s end-to-end latency.

![CI](https://github.com/andrecodea/agentic-deep-research/actions/workflows/ci.yml/badge.svg)

---

## Architecture

### Agent Flow

```
START
  ↓
Orchestrator — "intent == research?"
  ├── NO  → Respond to query → END
  └── YES ↓

Router — "new research or existing knowledge?"
  ├── EXISTING → Retriever (Chroma query) → Writer → END
  └── NEW ↓

Retriever (checks Chroma semantic cache)
  ├── HIT  → Writer
  └── MISS ↓

Researcher (Tavily search + extract)
  ↓
Writer (Markdown synthesis with images)
  ↓
HITL — "Save to knowledge base?"
  ├── YES → Retriever (Chroma embed + store) → END
  └── NO  → END
```

### State

```
messages        → conversation history (MessagesState reducer)
intent          → "research" | "conversation"
research_mode   → "new" | "existing"
query           → research query extracted from user input
retrieved_docs  → list[Document] from Chroma (add_docs reducer)
cache_hit       → whether relevant context was found in Chroma
search_results  → list[dict] from Tavily (add_docs reducer)
final_report    → Markdown report generated by the Writer
save_to_chroma  → HITL flag for knowledge base persistence
```

### Project Structure

```
agentic-deep-research/
├── src/
│   ├── utils/
│   │   ├── state.py        → ResearchState (shared state + reducers)
│   │   ├── tools.py        → Tavily search/extract, Chroma query/save
│   │   ├── nodes.py        → orchestrator, router, retriever, researcher, writer, hitl
│   │   ├── prompts.py      → LangSmith Hub prompt retrieval
│   │   └── vectorstore.py  → Chroma singleton initialization
│   └── agent.py            → StateGraph compilation + entry point
├── tests/                  → pytest unit tests
├── .github/
│   └── workflows/
│       └── ci.yml          → GitHub Actions (pytest + ruff)
├── Dockerfile
├── docker-compose.yml
├── langgraph.json
├── pyproject.toml
└── .env
```

## Stack

- **Orchestration**: LangGraph StateGraph
- **LLM**: Mercury 2 (Inception Labs) — 1,009 tokens/s, 1.7s end-to-end latency
- **Search**: Tavily (search + extract + images)
- **Memory**: Chroma (semantic cache via HITL) + PostgreSQL (session persistence)
- **Observability**: LangSmith (traces, PromptOps, session metrics)
- **Interface**: Agent Chat UI
- **Infrastructure**: Docker, GitHub Actions CI

## Setup

```bash
# Install dependencies
uv sync

# Configure environment
cp .env.example .env
# Add your API keys:
# INCEPTION_API_KEY
# TAVILY_API_KEY
# LANGSMITH_API_KEY
# POSTGRES_URI

# Run with Docker
docker compose up

# Or run locally
langgraph dev
```

Then open [Agent Chat UI](https://github.com/langchain-ai/agent-chat-ui) and point it to `http://localhost:2024`.

## Observability

All runs are traced in LangSmith with per-session metadata:

```
├── Tokens consumed per research session
├── Latency per node
├── Cache hit / miss rate
└── Estimated cost per run
```